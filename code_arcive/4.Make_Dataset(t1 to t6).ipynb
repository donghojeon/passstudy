{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 학습을 위한 데이터 구분 및 디렉토리 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pyjarowinkler import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'LearningData'\n",
    "\n",
    "clms = ['N/A','low', 'num','low&num', \n",
    "        'up', 'up&low','up&num', 'up&num&low',\n",
    "        'spc', 'spc&low', 'spc&num', 'spc&num&low',\n",
    "        'spc&up', 'spc&up&low', 'spc&up&num', 'all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_1 = [clms[1]]           # low -> low\n",
    "type_2 = [clms[1],clms[3]]   # low -> low&num\n",
    "type_3 = [clms[1],[clms[5],clms[9]]]   # low -> low&up, low&spc\n",
    "type_4 = [clms[1],[clms[7],clms[11],clms[13]]]   # low -> low&up&num, low&num&spc, low&up&spc\n",
    "type_5 = [clms[2]]           # num -> num   \n",
    "type_6 = [clms[2],clms[3]]   # num -> low&num\n",
    "\n",
    "extra_path = '/letter_specific_data'\n",
    "type_1_path = os.path.join(base_path, 'type_1'+extra_path)\n",
    "type_2_path = os.path.join(base_path, 'type_2'+extra_path)\n",
    "type_3_path = os.path.join(base_path, 'type_3'+extra_path)\n",
    "type_4_path = os.path.join(base_path, 'type_4'+extra_path)\n",
    "type_5_path = os.path.join(base_path, 'type_5'+extra_path)\n",
    "type_6_path = os.path.join(base_path, 'type_6'+extra_path)\n",
    "\n",
    "if not os.path.exists(type_1_path):os.makedirs(type_1_path)\n",
    "if not os.path.exists(type_2_path):os.makedirs(type_2_path)\n",
    "if not os.path.exists(type_3_path):os.makedirs(type_3_path)\n",
    "if not os.path.exists(type_4_path):os.makedirs(type_4_path)    \n",
    "if not os.path.exists(type_5_path):os.makedirs(type_5_path)\n",
    "if not os.path.exists(type_6_path):os.makedirs(type_6_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter = list(string.ascii_lowercase) +['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     20,
     32
    ]
   },
   "outputs": [],
   "source": [
    "def delete_single(src):\n",
    "    \n",
    "    if src <2:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return src\n",
    "    \n",
    "def SplitData(listin):\n",
    "    \n",
    "    length = len(listin)\n",
    "    listout = list()\n",
    "    \n",
    "    tmp = random.sample(listin, length)\n",
    "        \n",
    "    for i in range(length-1):\n",
    "        tmp_list = [tmp[0], tmp[i+1]]\n",
    "        listout.append(tmp_list)\n",
    "    \n",
    "    return listout\n",
    "\n",
    "def DropReindex(learn_data, TYPE):\n",
    "    \n",
    "    learn_data = learn_data.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data['length'] = learn_data[TYPE[0]].str.len()\n",
    "    learn_data['length'] = learn_data['length'].apply(delete_single)\n",
    "\n",
    "    learn_data = learn_data.dropna(how='any')\n",
    "    learn_data = learn_data.drop('length', axis = 1)\n",
    "    learn_data = learn_data.reset_index(drop=True)\n",
    "\n",
    "    return learn_data\n",
    "\n",
    "def ListExtend(listin):\n",
    "    \n",
    "    tmp = list()\n",
    "    \n",
    "    for i in range(len(listin)):\n",
    "        out = listin['Splitted'][i]\n",
    "        tmp.extend(out)\n",
    "        \n",
    "        if i%100000==0:\n",
    "            print(i)\n",
    "\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def DFandSave(learn_data, TYPE ,char):\n",
    "    \n",
    "    base = './LearningData/type_'+ str(TYPE)+ '/letter_specific_data/'\n",
    "    save_path = base+str(char)+'.pickle'\n",
    "    \n",
    "    data_columns = ['src', 'tgt']\n",
    "    pw_data = pd.DataFrame(learn_data, columns=data_columns)\n",
    "    pw_data.to_pickle(save_path)\n",
    "    \n",
    "    print(char, 'Data Saved', 'length=',len(pw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def SplitData_Multi(data_src, data_tgt):\n",
    "    \n",
    "    listout = list()\n",
    "    \n",
    "    for i in range(len(data_src)):\n",
    "        src = data_src[i]\n",
    "        \n",
    "        for j in range(len(data_tgt)):\n",
    "            tgt = data_tgt[j]\n",
    "            tmp = [src,tgt]\n",
    "            listout.append(tmp)\n",
    "    \n",
    "    return listout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 Split&Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Type1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_1]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = DropReindex(learn_data)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data['low'].apply(SplitData)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,1,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 올바른 전처리 결과 확인용\n",
    "pd.read_pickle('./LearningData/type_1/letter_specific_data/k.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 전처리된 데이터 a~z, num 들을 로드한 후 1개의 df로 결합\n",
    "\n",
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data load and Append')\n",
    "    \n",
    "    load_path = './LearningData/type_1/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## type 1\n",
    "save_path = type_1_path\n",
    "\n",
    "# 행의 값중 하나라도 nan인 경우 그 행을 없앤다., reindex\n",
    "#pw_data = pw_data.dropna(how='any').reset_index(drop=True)\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_1, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# 하나는 txt, 하나는 pickle로 저장한다.\n",
    "\n",
    "# src, tgt 모두 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_all.txt')\n",
    "pw_data.to_csv(path, header=False, index=False )\n",
    "# src, tgt 모두 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "###################################################################\n",
    "## split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.read_pickle('./LearningData/type_1/pw_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Type2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_2]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = learn_data.dropna(how='any').reset_index(drop=True)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_2[0]], x[type_2[1]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,2,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(pw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.read_pickle('./LearningData/type_2/letter_specific_data/k.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pw_data = pd.read_pickle('./Training_Inference/LearningData/type_2/pw_all.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data load and Append')\n",
    "    \n",
    "    load_path = './LearningData/type_2/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'low&num'] 갯수 : 72859274\n",
      "\n",
      "       src       tgt\n",
      "0  nikolaw  nikola69\n",
      "1  nikolaw  nikola11\n",
      "2  nikolaw  nikola91\n",
      "3  nikolaw  nikola19\n",
      "4  nikolaw  nikolaw9\n",
      "src -> tgt\n"
     ]
    }
   ],
   "source": [
    "## type 2\n",
    "save_path = type_2_path\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_2, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.txt')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "# df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "# df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# # train, valid, test 저장(txt)\n",
    "# path = os.path.join(save_path, 'pw_train.txt')\n",
    "# df_train.to_pickle(path)\n",
    "\n",
    "# path = os.path.join(save_path, 'pw_valid.txt')\n",
    "# df_valid.to_pickle(path)\n",
    "\n",
    "# path = os.path.join(save_path, 'pw_test.txt')\n",
    "# df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LearningData/type_2/letter_specific_data'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.core.internals.managers'; 'pandas.core.internals' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d7ef2faf6f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./LearningData/type_2/pw_test.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 return read_wrapper(\n\u001b[0;32m--> 175\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    147\u001b[0m                             is_text=False)\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_f\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 return read_wrapper(\n\u001b[0;32m--> 175\u001b[0;31m                     lambda f: pc.load(f, encoding=encoding, compat=True))\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/compat/pickle_compat.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fh, encoding, compat, is_verbose)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_verbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mload_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STACK_GLOBAL requires str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1385\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1386\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTACK_GLOBAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stack_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/compat/pickle_compat.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_class_locations_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_getattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas.core.internals.managers'; 'pandas.core.internals' is not a package"
     ]
    }
   ],
   "source": [
    "pd.read_pickle('./LearningData/type_2/pw_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Data Begin\n",
      "length of original learn data: 8770116\n",
      "length of processed learn data: 697460\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "a Data Saved length= 1695090\n",
      "\n",
      "b Data Begin\n",
      "length of original learn data: 4914853\n",
      "length of processed learn data: 354564\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "b Data Saved length= 870858\n",
      "\n",
      "c Data Begin\n",
      "length of original learn data: 4661423\n",
      "length of processed learn data: 329292\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "c Data Saved length= 864655\n",
      "\n",
      "d Data Begin\n",
      "length of original learn data: 5593731\n",
      "length of processed learn data: 424525\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "d Data Saved length= 1068340\n",
      "\n",
      "e Data Begin\n",
      "length of original learn data: 2715231\n",
      "length of processed learn data: 219626\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "e Data Saved length= 569694\n",
      "\n",
      "f Data Begin\n",
      "length of original learn data: 2582223\n",
      "length of processed learn data: 201128\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "f Data Saved length= 583050\n",
      "\n",
      "g Data Begin\n",
      "length of original learn data: 3284478\n",
      "length of processed learn data: 270796\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "g Data Saved length= 720185\n",
      "\n",
      "h Data Begin\n",
      "length of original learn data: 2382683\n",
      "length of processed learn data: 164892\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "h Data Saved length= 420399\n",
      "\n",
      "i Data Begin\n",
      "length of original learn data: 2093863\n",
      "length of processed learn data: 167565\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "i Data Saved length= 432901\n",
      "\n",
      "j Data Begin\n",
      "length of original learn data: 4720419\n",
      "length of processed learn data: 293135\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "j Data Saved length= 695772\n",
      "\n",
      "k Data Begin\n",
      "length of original learn data: 4982040\n",
      "length of processed learn data: 392713\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "k Data Saved length= 961438\n",
      "\n",
      "l Data Begin\n",
      "length of original learn data: 4680674\n",
      "length of processed learn data: 347709\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "l Data Saved length= 940135\n",
      "\n",
      "m Data Begin\n",
      "length of original learn data: 8157479\n",
      "length of processed learn data: 660744\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "m Data Saved length= 1698222\n",
      "\n",
      "n Data Begin\n",
      "length of original learn data: 3498824\n",
      "length of processed learn data: 297903\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "n Data Saved length= 745229\n",
      "\n",
      "o Data Begin\n",
      "length of original learn data: 1508427\n",
      "length of processed learn data: 132095\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "o Data Saved length= 327205\n",
      "\n",
      "p Data Begin\n",
      "length of original learn data: 3667612\n",
      "length of processed learn data: 295653\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "p Data Saved length= 771370\n",
      "\n",
      "q Data Begin\n",
      "length of original learn data: 379237\n",
      "length of processed learn data: 22434\n",
      "Data Splitted\n",
      "0\n",
      "q Data Saved length= 58973\n",
      "\n",
      "r Data Begin\n",
      "length of original learn data: 4025951\n",
      "length of processed learn data: 301124\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "r Data Saved length= 715154\n",
      "\n",
      "s Data Begin\n",
      "length of original learn data: 8751026\n",
      "length of processed learn data: 700837\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "s Data Saved length= 1741493\n",
      "\n",
      "t Data Begin\n",
      "length of original learn data: 4021928\n",
      "length of processed learn data: 277507\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "t Data Saved length= 692045\n",
      "\n",
      "u Data Begin\n",
      "length of original learn data: 578985\n",
      "length of processed learn data: 43944\n",
      "Data Splitted\n",
      "0\n",
      "u Data Saved length= 116668\n",
      "\n",
      "v Data Begin\n",
      "length of original learn data: 2579178\n",
      "length of processed learn data: 235192\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "v Data Saved length= 605503\n",
      "\n",
      "w Data Begin\n",
      "length of original learn data: 1800892\n",
      "length of processed learn data: 97760\n",
      "Data Splitted\n",
      "0\n",
      "w Data Saved length= 226903\n",
      "\n",
      "x Data Begin\n",
      "length of original learn data: 696787\n",
      "length of processed learn data: 37238\n",
      "Data Splitted\n",
      "0\n",
      "x Data Saved length= 81432\n",
      "\n",
      "y Data Begin\n",
      "length of original learn data: 1163990\n",
      "length of processed learn data: 76566\n",
      "Data Splitted\n",
      "0\n",
      "y Data Saved length= 183610\n",
      "\n",
      "z Data Begin\n",
      "length of original learn data: 1246548\n",
      "length of processed learn data: 98114\n",
      "Data Splitted\n",
      "0\n",
      "z Data Saved length= 257503\n",
      "\n",
      "num Data Begin\n",
      "length of original learn data: 2923676\n",
      "length of processed learn data: 82647\n",
      "Data Splitted\n",
      "0\n",
      "num Data Saved length= 244152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data1 = pw_df[[type_3[0], type_3[1][0]]]\n",
    "    learn_data2 = pw_df[[type_3[0], type_3[1][1]]]\n",
    "    print('length of original learn data:',len(learn_data1))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data1 = learn_data1.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data2 = learn_data2.dropna(how='any').reset_index(drop=True)\n",
    "    \n",
    "    learn_data2.rename(columns = {'spc&low':'up&low'}, inplace=True)\n",
    "    learn_data = learn_data1.append(learn_data2)\n",
    "    learn_data = learn_data.reset_index(drop=True)\n",
    "    \n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_3[0]], x[type_3[1][0]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,3,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caberboo</td>\n",
       "      <td>Ciberbox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eesmpinutile</td>\n",
       "      <td>Esmpinutile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esmpinutilee</td>\n",
       "      <td>Esmpinutile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>akilina</td>\n",
       "      <td>Akilina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gfifdjkz</td>\n",
       "      <td>Gfifdjkz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864650</th>\n",
       "      <td>soccerabc</td>\n",
       "      <td>soccer!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864651</th>\n",
       "      <td>mmsocceraa</td>\n",
       "      <td>soccer!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864652</th>\n",
       "      <td>czerwonyptak</td>\n",
       "      <td>czerwony.gnom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864653</th>\n",
       "      <td>czerwonygnom</td>\n",
       "      <td>czerwony.gnom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864654</th>\n",
       "      <td>czerwony</td>\n",
       "      <td>czerwony.gnom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>864655 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 src            tgt\n",
       "0           caberboo       Ciberbox\n",
       "1       eesmpinutile    Esmpinutile\n",
       "2       esmpinutilee    Esmpinutile\n",
       "3            akilina        Akilina\n",
       "4           gfifdjkz       Gfifdjkz\n",
       "...              ...            ...\n",
       "864650     soccerabc        soccer!\n",
       "864651    mmsocceraa        soccer!\n",
       "864652  czerwonyptak  czerwony.gnom\n",
       "864653  czerwonygnom  czerwony.gnom\n",
       "864654      czerwony  czerwony.gnom\n",
       "\n",
       "[864655 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('./LearningData/type_3/letter_specific_data/c.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Data\n",
      "1695090\n",
      "b Data\n",
      "870858\n",
      "c Data\n",
      "864655\n",
      "d Data\n",
      "1068340\n",
      "e Data\n",
      "569694\n",
      "f Data\n",
      "583050\n",
      "g Data\n",
      "720185\n",
      "h Data\n",
      "420399\n",
      "i Data\n",
      "432901\n",
      "j Data\n",
      "695772\n",
      "k Data\n",
      "961438\n",
      "l Data\n",
      "940135\n",
      "m Data\n",
      "1698222\n",
      "n Data\n",
      "745229\n",
      "o Data\n",
      "327205\n",
      "p Data\n",
      "771370\n",
      "q Data\n",
      "58973\n",
      "r Data\n",
      "715154\n",
      "s Data\n",
      "1741493\n",
      "t Data\n",
      "692045\n",
      "u Data\n",
      "116668\n",
      "v Data\n",
      "605503\n",
      "w Data\n",
      "226903\n",
      "x Data\n",
      "81432\n",
      "y Data\n",
      "183610\n",
      "z Data\n",
      "257503\n",
      "num Data\n",
      "244152\n"
     ]
    }
   ],
   "source": [
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data')\n",
    "    \n",
    "    load_path = './LearningData/type_3/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18287979"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', ['up&low', 'spc&low']] 갯수 : 18287979\n",
      "\n",
      "                    src         tgt\n",
      "0                aidana      Aidana\n",
      "1                aidana      Aidana\n",
      "2                aidana      Aidana\n",
      "3  saifulazrisaifulazri  Saifulazri\n",
      "4            saifulazri  Saifulazri\n",
      "src -> tgt\n"
     ]
    }
   ],
   "source": [
    "## type 3\n",
    "save_path = type_3_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_3, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'low'  ->  'up&num&low', 'spc&num&low', 'spc&up&low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_4 = [clms[1],[clms[7],clms[11],clms[13]]]   # low -> low&up&num, low&num&spc, low&up&spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b Data Begin\n",
      "length of original learn data: 4914853\n",
      "length of processed learn data: 266475\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "b Data Saved length= 490938\n",
      "\n",
      "c Data Begin\n",
      "length of original learn data: 4661423\n",
      "length of processed learn data: 244713\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "c Data Saved length= 451386\n",
      "\n",
      "d Data Begin\n",
      "length of original learn data: 5593731\n",
      "length of processed learn data: 336696\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "d Data Saved length= 647038\n",
      "\n",
      "e Data Begin\n",
      "length of original learn data: 2715231\n",
      "length of processed learn data: 144759\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "e Data Saved length= 275029\n",
      "\n",
      "f Data Begin\n",
      "length of original learn data: 2582223\n",
      "length of processed learn data: 134653\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "f Data Saved length= 247656\n",
      "\n",
      "g Data Begin\n",
      "length of original learn data: 3284478\n",
      "length of processed learn data: 194932\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "g Data Saved length= 364102\n",
      "\n",
      "h Data Begin\n",
      "length of original learn data: 2382683\n",
      "length of processed learn data: 113786\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "h Data Saved length= 193338\n",
      "\n",
      "i Data Begin\n",
      "length of original learn data: 2093863\n",
      "length of processed learn data: 106921\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "i Data Saved length= 196451\n",
      "\n",
      "j Data Begin\n",
      "length of original learn data: 4720419\n",
      "length of processed learn data: 224123\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "j Data Saved length= 364371\n",
      "\n",
      "k Data Begin\n",
      "length of original learn data: 4982040\n",
      "length of processed learn data: 291977\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "k Data Saved length= 534378\n",
      "\n",
      "l Data Begin\n",
      "length of original learn data: 4680674\n",
      "length of processed learn data: 225381\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "l Data Saved length= 401945\n",
      "\n",
      "m Data Begin\n",
      "length of original learn data: 8157479\n",
      "length of processed learn data: 468540\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "m Data Saved length= 848450\n",
      "\n",
      "n Data Begin\n",
      "length of original learn data: 3498824\n",
      "length of processed learn data: 219407\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "n Data Saved length= 408557\n",
      "\n",
      "o Data Begin\n",
      "length of original learn data: 1508427\n",
      "length of processed learn data: 79559\n",
      "Data Splitted\n",
      "0\n",
      "o Data Saved length= 145351\n",
      "\n",
      "p Data Begin\n",
      "length of original learn data: 3667612\n",
      "length of processed learn data: 205990\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "p Data Saved length= 371446\n",
      "\n",
      "q Data Begin\n",
      "length of original learn data: 379237\n",
      "length of processed learn data: 20536\n",
      "Data Splitted\n",
      "0\n",
      "q Data Saved length= 42571\n",
      "\n",
      "r Data Begin\n",
      "length of original learn data: 4025951\n",
      "length of processed learn data: 221641\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "r Data Saved length= 383178\n",
      "\n",
      "s Data Begin\n",
      "length of original learn data: 8751026\n",
      "length of processed learn data: 519698\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "s Data Saved length= 937347\n",
      "\n",
      "t Data Begin\n",
      "length of original learn data: 4021928\n",
      "length of processed learn data: 199572\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "t Data Saved length= 354075\n",
      "\n",
      "u Data Begin\n",
      "length of original learn data: 578985\n",
      "length of processed learn data: 26338\n",
      "Data Splitted\n",
      "0\n",
      "u Data Saved length= 48070\n",
      "\n",
      "v Data Begin\n",
      "length of original learn data: 2579178\n",
      "length of processed learn data: 163689\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "v Data Saved length= 307436\n",
      "\n",
      "w Data Begin\n",
      "length of original learn data: 1800892\n",
      "length of processed learn data: 69546\n",
      "Data Splitted\n",
      "0\n",
      "w Data Saved length= 117017\n",
      "\n",
      "x Data Begin\n",
      "length of original learn data: 696787\n",
      "length of processed learn data: 23339\n",
      "Data Splitted\n",
      "0\n",
      "x Data Saved length= 39084\n",
      "\n",
      "y Data Begin\n",
      "length of original learn data: 1163990\n",
      "length of processed learn data: 50005\n",
      "Data Splitted\n",
      "0\n",
      "y Data Saved length= 87186\n",
      "\n",
      "z Data Begin\n",
      "length of original learn data: 1246548\n",
      "length of processed learn data: 69513\n",
      "Data Splitted\n",
      "0\n",
      "z Data Saved length= 139404\n",
      "\n",
      "num Data Begin\n",
      "length of original learn data: 2923676\n",
      "length of processed learn data: 37667\n",
      "Data Splitted\n",
      "0\n",
      "num Data Saved length= 64203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data1 = pw_df[[type_4[0], type_4[1][0]]]\n",
    "    learn_data2 = pw_df[[type_4[0], type_4[1][1]]]\n",
    "    learn_data3 = pw_df[[type_4[0], type_4[1][2]]]\n",
    "    print('length of original learn data:',len(learn_data1))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data1 = learn_data1.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data2 = learn_data2.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data3 = learn_data3.dropna(how='any').reset_index(drop=True)\n",
    "    \n",
    "    learn_data2.rename(columns = {'spc&num&low':'up&num&low'}, inplace=True)\n",
    "    learn_data3.rename(columns = { 'spc&up&low':'up&num&low'}, inplace=True)\n",
    "    \n",
    "    learn_data = learn_data1.append(learn_data2)\n",
    "    learn_data = learn_data.append(learn_data3)\n",
    "    learn_data = learn_data.reset_index(drop=True)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_4[0]], x[type_4[1][0]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,4,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asdzxc</td>\n",
       "      <td>Asdzxc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asdzxc</td>\n",
       "      <td>asdzxc1Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vmogile</td>\n",
       "      <td>Vmogile2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kimgorgv</td>\n",
       "      <td>22KIMgorGV35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whatever</td>\n",
       "      <td>Whatever4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364097</th>\n",
       "      <td>gysoftq</td>\n",
       "      <td>Gy_soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364098</th>\n",
       "      <td>gysoftq</td>\n",
       "      <td>Gy_softa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364099</th>\n",
       "      <td>gyamfua</td>\n",
       "      <td>Gyamfuah@yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364100</th>\n",
       "      <td>gyamfuaa</td>\n",
       "      <td>Gyamfuah@yahoo.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364101</th>\n",
       "      <td>gyamfuaa</td>\n",
       "      <td>Gyamfuah@yahoo.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364102 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             src                 tgt\n",
       "0         asdzxc             Asdzxc1\n",
       "1         asdzxc            asdzxc1Q\n",
       "2        vmogile         Vmogile2011\n",
       "3       kimgorgv        22KIMgorGV35\n",
       "4       whatever           Whatever4\n",
       "...          ...                 ...\n",
       "364097   gysoftq             Gy_soft\n",
       "364098   gysoftq            Gy_softa\n",
       "364099   gyamfua  Gyamfuah@yahoo.com\n",
       "364100  gyamfuaa  Gyamfuah@yahoo.com\n",
       "364101  gyamfuaa  Gyamfuah@yahoo.com\n",
       "\n",
       "[364102 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('./LearningData/type_4/letter_specific_data/g.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Data\n",
      "1004286\n",
      "b Data\n",
      "490938\n",
      "c Data\n",
      "451386\n",
      "d Data\n",
      "647038\n",
      "e Data\n",
      "275029\n",
      "f Data\n",
      "247656\n",
      "g Data\n",
      "364102\n",
      "h Data\n",
      "193338\n",
      "i Data\n",
      "196451\n",
      "j Data\n",
      "364371\n",
      "k Data\n",
      "534378\n",
      "l Data\n",
      "401945\n",
      "m Data\n",
      "848450\n",
      "n Data\n",
      "408557\n",
      "o Data\n",
      "145351\n",
      "p Data\n",
      "371446\n",
      "q Data\n",
      "42571\n",
      "r Data\n",
      "383178\n",
      "s Data\n",
      "937347\n",
      "t Data\n",
      "354075\n",
      "u Data\n",
      "48070\n",
      "v Data\n",
      "307436\n",
      "w Data\n",
      "117017\n",
      "x Data\n",
      "39084\n",
      "y Data\n",
      "87186\n",
      "z Data\n",
      "139404\n",
      "num Data\n",
      "64203\n"
     ]
    }
   ],
   "source": [
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data')\n",
    "    \n",
    "    load_path = './LearningData/type_4/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9464293"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', ['up&num&low', 'spc&num&low', 'spc&up&low']] 갯수 : 9464293\n",
      "\n",
      "        src           tgt\n",
      "0  aidardia  aiDAR7aidar5\n",
      "1  aidardia  aiDAR7aidar5\n",
      "2  aidardia  aiDAR7aidar5\n",
      "3  barabash  Barabash2011\n",
      "4   android     Android06\n",
      "src -> tgt\n"
     ]
    }
   ],
   "source": [
    "## type 4\n",
    "save_path = type_4_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_4, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Type5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pw_df['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_5]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = DropReindex(learn_data, type_5)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data['num'].apply(SplitData)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,5,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 올바른 전처리 결과 확인용\n",
    "pd.read_pickle('./LearningData/type_5/letter_specific_data/k.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 전처리된 데이터 a~z, num 들을 로드한 후 1개의 df로 결합\n",
    "\n",
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data load and Append')\n",
    "    \n",
    "    load_path = './LearningData/type_5/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## type 5\n",
    "save_path = type_5_path\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_5, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 모두 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "########################################################## split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Type6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Data Begin\n",
      "length of original learn data: 8770116\n",
      "length of processed learn data: 816899\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "a Data Saved length= 3685846\n",
      "\n",
      "b Data Begin\n",
      "length of original learn data: 4914853\n",
      "length of processed learn data: 318057\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "b Data Saved length= 1138368\n",
      "\n",
      "c Data Begin\n",
      "length of original learn data: 4661423\n",
      "length of processed learn data: 258065\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "c Data Saved length= 908324\n",
      "\n",
      "d Data Begin\n",
      "length of original learn data: 5593731\n",
      "length of processed learn data: 419694\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "d Data Saved length= 1461795\n",
      "\n",
      "e Data Begin\n",
      "length of original learn data: 2715231\n",
      "length of processed learn data: 211606\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "e Data Saved length= 748726\n",
      "\n",
      "f Data Begin\n",
      "length of original learn data: 2582223\n",
      "length of processed learn data: 203776\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "f Data Saved length= 746754\n",
      "\n",
      "g Data Begin\n",
      "length of original learn data: 3284478\n",
      "length of processed learn data: 259144\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "g Data Saved length= 950309\n",
      "\n",
      "h Data Begin\n",
      "length of original learn data: 2382683\n",
      "length of processed learn data: 181770\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "h Data Saved length= 590339\n",
      "\n",
      "i Data Begin\n",
      "length of original learn data: 2093863\n",
      "length of processed learn data: 188851\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "i Data Saved length= 655794\n",
      "\n",
      "j Data Begin\n",
      "length of original learn data: 4720419\n",
      "length of processed learn data: 231210\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "j Data Saved length= 737214\n",
      "\n",
      "k Data Begin\n",
      "length of original learn data: 4982040\n",
      "length of processed learn data: 410985\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "k Data Saved length= 1492580\n",
      "\n",
      "l Data Begin\n",
      "length of original learn data: 4680674\n",
      "length of processed learn data: 349466\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "l Data Saved length= 1226515\n",
      "\n",
      "m Data Begin\n",
      "length of original learn data: 8157479\n",
      "length of processed learn data: 568886\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "m Data Saved length= 2081044\n",
      "\n",
      "n Data Begin\n",
      "length of original learn data: 3498824\n",
      "length of processed learn data: 295204\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "n Data Saved length= 1082010\n",
      "\n",
      "o Data Begin\n",
      "length of original learn data: 1508427\n",
      "length of processed learn data: 144142\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "o Data Saved length= 511483\n",
      "\n",
      "p Data Begin\n",
      "length of original learn data: 3667612\n",
      "length of processed learn data: 257201\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "p Data Saved length= 894682\n",
      "\n",
      "q Data Begin\n",
      "length of original learn data: 379237\n",
      "length of processed learn data: 47092\n",
      "Data Splitted\n",
      "0\n",
      "q Data Saved length= 136127\n",
      "\n",
      "r Data Begin\n",
      "length of original learn data: 4025951\n",
      "length of processed learn data: 265950\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "r Data Saved length= 891575\n",
      "\n",
      "s Data Begin\n",
      "length of original learn data: 8751026\n",
      "length of processed learn data: 658374\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "s Data Saved length= 2296091\n",
      "\n",
      "t Data Begin\n",
      "length of original learn data: 4021928\n",
      "length of processed learn data: 277007\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "t Data Saved length= 908942\n",
      "\n",
      "u Data Begin\n",
      "length of original learn data: 578985\n",
      "length of processed learn data: 56292\n",
      "Data Splitted\n",
      "0\n",
      "u Data Saved length= 197513\n",
      "\n",
      "v Data Begin\n",
      "length of original learn data: 2579178\n",
      "length of processed learn data: 258785\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "v Data Saved length= 933019\n",
      "\n",
      "w Data Begin\n",
      "length of original learn data: 1800892\n",
      "length of processed learn data: 155992\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "w Data Saved length= 440987\n",
      "\n",
      "x Data Begin\n",
      "length of original learn data: 696787\n",
      "length of processed learn data: 68010\n",
      "Data Splitted\n",
      "0\n",
      "x Data Saved length= 183499\n",
      "\n",
      "y Data Begin\n",
      "length of original learn data: 1163990\n",
      "length of processed learn data: 122022\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "y Data Saved length= 362702\n",
      "\n",
      "z Data Begin\n",
      "length of original learn data: 1246548\n",
      "length of processed learn data: 139387\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "z Data Saved length= 485979\n",
      "\n",
      "num Data Begin\n",
      "length of original learn data: 2923676\n",
      "length of processed learn data: 598164\n",
      "Data Splitted\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "num Data Saved length= 1836728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_6]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = learn_data.dropna(how='any').reset_index(drop=True)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_6[0]], x[type_6[1]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,6,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19800222</td>\n",
       "      <td>198005kdw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123456</td>\n",
       "      <td>123456qazwsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123456</td>\n",
       "      <td>1123456qazwsx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123456</td>\n",
       "      <td>123456qazwsxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123456</td>\n",
       "      <td>123456qazwsx1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492575</th>\n",
       "      <td>617383</td>\n",
       "      <td>617383bogdan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492576</th>\n",
       "      <td>02071988</td>\n",
       "      <td>alex02071988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492577</th>\n",
       "      <td>02071988</td>\n",
       "      <td>alex12121688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492578</th>\n",
       "      <td>02071988</td>\n",
       "      <td>alex020719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492579</th>\n",
       "      <td>02071988</td>\n",
       "      <td>alex020719881988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1492580 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              src               tgt\n",
       "0        19800222         198005kdw\n",
       "1          123456      123456qazwsx\n",
       "2          123456     1123456qazwsx\n",
       "3          123456     123456qazwsxx\n",
       "4          123456     123456qazwsx1\n",
       "...           ...               ...\n",
       "1492575    617383      617383bogdan\n",
       "1492576  02071988      alex02071988\n",
       "1492577  02071988      alex12121688\n",
       "1492578  02071988        alex020719\n",
       "1492579  02071988  alex020719881988\n",
       "\n",
       "[1492580 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('./LearningData/type_6/letter_specific_data/k.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Data\n",
      "3685846\n",
      "b Data\n",
      "1138368\n",
      "c Data\n",
      "908324\n",
      "d Data\n",
      "1461795\n",
      "e Data\n",
      "748726\n",
      "f Data\n",
      "746754\n",
      "g Data\n",
      "950309\n",
      "h Data\n",
      "590339\n",
      "i Data\n",
      "655794\n",
      "j Data\n",
      "737214\n",
      "k Data\n",
      "1492580\n",
      "l Data\n",
      "1226515\n",
      "m Data\n",
      "2081044\n",
      "n Data\n",
      "1082010\n",
      "o Data\n",
      "511483\n",
      "p Data\n",
      "894682\n",
      "q Data\n",
      "136127\n",
      "r Data\n",
      "891575\n",
      "s Data\n",
      "2296091\n",
      "t Data\n",
      "908942\n",
      "u Data\n",
      "197513\n",
      "v Data\n",
      "933019\n",
      "w Data\n",
      "440987\n",
      "x Data\n",
      "183499\n",
      "y Data\n",
      "362702\n",
      "z Data\n",
      "485979\n",
      "num Data\n",
      "1836728\n"
     ]
    }
   ],
   "source": [
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data')\n",
    "    \n",
    "    load_path = './LearningData/type_6/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27584945"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num', 'low&num'] 갯수 : 27584945\n",
      "\n",
      "           src               tgt\n",
      "0   1234567890        15678902q1\n",
      "1    123456789       ai123456789\n",
      "2    123456789  aidos12345678901\n",
      "3    123456789        a123456789\n",
      "4  12345678910       ai123456789\n",
      "src -> tgt\n"
     ]
    }
   ],
   "source": [
    "## type 6\n",
    "save_path = type_6_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_6, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
