{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pyjarowinkler import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_1]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = DropReindex(learn_data)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data['low'].apply(SplitData)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,1,char)\n",
    "    print('')\n",
    "\n",
    "    # 전처리된 데이터 a~z, num 들을 로드한 후 1개의 df로 결합\n",
    "\n",
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data load and Append')\n",
    "    \n",
    "    load_path = './LearningData/type_1/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 1\n",
    "save_path = type_1_path\n",
    "\n",
    "# 행의 값중 하나라도 nan인 경우 그 행을 없앤다., reindex\n",
    "#pw_data = pw_data.dropna(how='any').reset_index(drop=True)\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_1, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# 하나는 txt, 하나는 pickle로 저장한다.\n",
    "\n",
    "# src, tgt 모두 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_all.txt')\n",
    "pw_data.to_csv(path, header=False, index=False )\n",
    "# src, tgt 모두 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "###################################################################\n",
    "## split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SplitData_Multi\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_2]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = learn_data.dropna(how='any').reset_index(drop=True)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_2[0]], x[type_2[1]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,2,char)\n",
    "    print('')\n",
    "    \n",
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data load and Append')\n",
    "    \n",
    "    load_path = './LearningData/type_2/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 2\n",
    "save_path = type_2_path\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_2, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.txt')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "# df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "# df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# # train, valid, test 저장(txt)\n",
    "# path = os.path.join(save_path, 'pw_train.txt')\n",
    "# df_train.to_pickle(path)\n",
    "\n",
    "# path = os.path.join(save_path, 'pw_valid.txt')\n",
    "# df_valid.to_pickle(path)\n",
    "\n",
    "# path = os.path.join(save_path, 'pw_test.txt')\n",
    "# df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data1 = pw_df[[type_3[0], type_3[1][0]]]\n",
    "    learn_data2 = pw_df[[type_3[0], type_3[1][1]]]\n",
    "    print('length of original learn data:',len(learn_data1))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data1 = learn_data1.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data2 = learn_data2.dropna(how='any').reset_index(drop=True)\n",
    "    \n",
    "    learn_data2.rename(columns = {'spc&low':'up&low'}, inplace=True)\n",
    "    learn_data = learn_data1.append(learn_data2)\n",
    "    learn_data = learn_data.reset_index(drop=True)\n",
    "    \n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_3[0]], x[type_3[1][0]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,3,char)\n",
    "    print('')\n",
    "\n",
    "    data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data')\n",
    "    \n",
    "    load_path = './LearningData/type_3/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 3\n",
    "save_path = type_3_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_3, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data1 = pw_df[[type_4[0], type_4[1][0]]]\n",
    "    learn_data2 = pw_df[[type_4[0], type_4[1][1]]]\n",
    "    learn_data3 = pw_df[[type_4[0], type_4[1][2]]]\n",
    "    print('length of original learn data:',len(learn_data1))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data1 = learn_data1.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data2 = learn_data2.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data3 = learn_data3.dropna(how='any').reset_index(drop=True)\n",
    "    \n",
    "    learn_data2.rename(columns = {'spc&num&low':'up&num&low'}, inplace=True)\n",
    "    learn_data3.rename(columns = { 'spc&up&low':'up&num&low'}, inplace=True)\n",
    "    \n",
    "    learn_data = learn_data1.append(learn_data2)\n",
    "    learn_data = learn_data.append(learn_data3)\n",
    "    learn_data = learn_data.reset_index(drop=True)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_4[0]], x[type_4[1][0]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,4,char)\n",
    "    print('')\n",
    "\n",
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data')\n",
    "    \n",
    "    load_path = './LearningData/type_4/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 4\n",
    "save_path = type_4_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_4, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_5]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = DropReindex(learn_data, type_5)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data['num'].apply(SplitData)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,5,char)\n",
    "    print('')\n",
    "# 전처리된 데이터 a~z, num 들을 로드한 후 1개의 df로 결합\n",
    "\n",
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data load and Append')\n",
    "    \n",
    "    load_path = './LearningData/type_5/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 5\n",
    "save_path = type_5_path\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_5, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 모두 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "########################################################## split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_6]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = learn_data.dropna(how='any').reset_index(drop=True)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_6[0]], x[type_6[1]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,6,char)\n",
    "    print('')\n",
    "\n",
    "data_columns = ['src', 'tgt']\n",
    "pw_data = pd.DataFrame(columns=data_columns)\n",
    "\n",
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data')\n",
    "    \n",
    "    load_path = './LearningData/type_6/letter_specific_data/'+str(char)+'.pickle'\n",
    "    temp = pd.read_pickle(load_path)\n",
    "    \n",
    "    print(len(temp))\n",
    "    pw_data = pw_data.append(temp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 6\n",
    "save_path = type_6_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_6, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 7\n",
    "save_path = type_7_path[:-20]\n",
    "print('%s 갯수 : %d\\n' % (type_7, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 모두 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data1 = pw_df[[type_8[0], type_8[1][0]]]\n",
    "    learn_data2 = pw_df[[type_8[0], type_8[1][1]]]\n",
    "    print('length of original learn data:',len(learn_data1))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data1 = learn_data1.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data2 = learn_data2.dropna(how='any').reset_index(drop=True)\n",
    "    \n",
    "    learn_data2.rename(columns = {'spc&num&low':'up&num&low'}, inplace=True)\n",
    "    learn_data = learn_data1.append(learn_data2)\n",
    "    learn_data = learn_data.reset_index(drop=True)\n",
    "    \n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_8[0]], x[type_8[1][0]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,8,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 8\n",
    "save_path = type_8_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_8, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data1 = pw_df[[type_9[0], type_9[1][0]]]\n",
    "    learn_data2 = pw_df[[type_9[0], type_9[1][1]]]\n",
    "    print('length of original learn data:',len(learn_data1))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data1 = learn_data1.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data2 = learn_data2.dropna(how='any').reset_index(drop=True)\n",
    "    \n",
    "    learn_data2.rename(columns = {'spc&up&low':'up&num&low'}, inplace=True)\n",
    "    learn_data = learn_data1.append(learn_data2)\n",
    "    learn_data = learn_data.reset_index(drop=True)\n",
    "    \n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_9[0]], x[type_9[1][0]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,9,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 9\n",
    "save_path = type_9_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_9, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data1 = pw_df[[type_10[0], type_10[1][0]]]\n",
    "    learn_data2 = pw_df[[type_10[0], type_10[1][1]]]\n",
    "    print('length of original learn data:',len(learn_data1))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data1 = learn_data1.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data2 = learn_data2.dropna(how='any').reset_index(drop=True)\n",
    "    \n",
    "    learn_data2.rename(columns = {'spc&up&low':'spc&num&low'}, inplace=True)\n",
    "    learn_data = learn_data1.append(learn_data2)\n",
    "    learn_data = learn_data.reset_index(drop=True)\n",
    "    \n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_10[0]], x[type_10[1][0]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,10,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 10\n",
    "save_path = type_10_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_10, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_11]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = learn_data.dropna(how='any').reset_index(drop=True)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_11[0]], x[type_11[1]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,11,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 11\n",
    "save_path = type_11_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_11, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_12]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = DropReindex(learn_data, type_12)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data['up&num&low'].apply(SplitData)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,12,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 12\n",
    "save_path = type_12_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_12, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,27):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data = pw_df[type_13]\n",
    "    print('length of original learn data:',len(learn_data))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data = DropReindex(learn_data, type_13)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data['spc&num&low'].apply(SplitData)\n",
    "    print('Data Splitted')\n",
    "    \n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,13,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 13\n",
    "save_path = type_13_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_13, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,1):\n",
    "    char = letter[i]\n",
    "    print(char, 'Data Begin')\n",
    "    \n",
    "    # 데이터 로드\n",
    "    load_path = './Preprocessed/' +str(char)+'.pickle'\n",
    "    pw_df = pd.read_pickle(load_path)\n",
    "    \n",
    "    # 원하는 타입 추출\n",
    "    learn_data1 = pw_df[[type_14[0][0], type_14[1]]]\n",
    "    learn_data2 = pw_df[[type_14[0][1], type_14[1]]]\n",
    "    learn_data3 = pw_df[[type_14[0][2], type_14[1]]]\n",
    "    learn_data4 = pw_df[[type_14[0][3], type_14[1]]]\n",
    "    print('length of original learn data:',len(learn_data1))\n",
    "    \n",
    "    # 불필요한 데이터 drop 및 reindex\n",
    "    learn_data1 = learn_data1.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data2 = learn_data2.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data3 = learn_data3.dropna(how='any').reset_index(drop=True)\n",
    "    learn_data4 = learn_data4.dropna(how='any').reset_index(drop=True)\n",
    "\n",
    "    learn_data2.rename(columns = {'spc&num&low':'up&num&low'}, inplace=True)\n",
    "    learn_data3.rename(columns = { 'spc&up&low':'up&num&low'}, inplace=True)\n",
    "    learn_data4.rename(columns = { 'spc&up&num':'up&num&low'}, inplace=True)\n",
    "\n",
    "    learn_data = learn_data1.append(learn_data2)\n",
    "    learn_data = learn_data.append(learn_data3)\n",
    "    learn_data = learn_data.append(learn_data4)\n",
    "\n",
    "    learn_data = learn_data.reset_index(drop=True)\n",
    "    print('length of processed learn data:',len(learn_data))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 길이가 2 이상인 데이터 나누기\n",
    "    learn_data['Splitted'] = learn_data.apply(lambda x:SplitData_Multi(x[type_14[0][0]], x[type_14[1]]), axis = 1)\n",
    "    print('Data Splitted')\n",
    "\n",
    "    # 나뉘어진 데이터 이어 붙이기 \n",
    "    learn_data= ListExtend(learn_data)\n",
    "    \n",
    "    # src, tgt 데이터 프래임 생성 및 파일 저장\n",
    "    DFandSave(learn_data,14,char)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## type 14\n",
    "save_path = type_14_path[:-20]\n",
    "\n",
    "print('%s 갯수 : %d\\n' % (type_14, len(pw_data)))\n",
    "print(pw_data[:5])\n",
    "\n",
    "file_name_src = pw_data.columns[0]\n",
    "file_name_tgt = pw_data.columns[1]\n",
    "print('%s -> %s'%(file_name_src, file_name_tgt))\n",
    "\n",
    "# src, tgt 전체 데이터 저장(csv)\n",
    "path1 = os.path.join(save_path, 'pw_all.pickle')\n",
    "pw_data.to_pickle(path1)  \n",
    "\n",
    "################################################################ split\n",
    "df_train, df_tmp = train_test_split(pw_data, test_size=0.2)\n",
    "df_valid, df_test = train_test_split(df_tmp, test_size=0.5)\n",
    "\n",
    "# train, valid, test 저장(txt)\n",
    "path = os.path.join(save_path, 'pw_train.pickle')\n",
    "df_train.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_valid.pickle')\n",
    "df_valid.to_pickle(path)\n",
    "\n",
    "path = os.path.join(save_path, 'pw_test.pickle')\n",
    "df_test.to_pickle(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
