{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import string\n",
    "import math\n",
    "import time\n",
    "\n",
    "from itertools import groupby\n",
    "from pprint import pprint as pp\n",
    "from pyjarowinkler import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# available char 는 ASCII 코드 33 ~ 126 까지의 값들의 모임이다.\n",
    "\n",
    "available_char = string.digits+string.ascii_letters+string.punctuation\n",
    "print(available_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Available(string):\n",
    "    for element in string:\n",
    "        if element not in available_char:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['ID','Domain', 'PW']\n",
    "\n",
    "\n",
    "for char in string.ascii_letters:\n",
    "    \n",
    "    \n",
    "    path = './3Billion/' +char + '-sorted-uniq.csv'\n",
    "    print(path)\n",
    "    \n",
    "    data = pd.read_csv(path, names = colnames, header=None, error_bad_lines=False)\n",
    "        \n",
    "    print(len(data), '<- data의 원래 길이:' )\n",
    "    \n",
    "    data = data.dropna(axis=0)\n",
    "    print(len(data), 'NaN제거후의 길이')\n",
    "    \n",
    "    data['PW'] = data['PW'].apply(Available)\n",
    "    data = data.dropna(how='any')\n",
    "    print(len(data),'<- PW 칼럼에서 noise제거후 길이' )\n",
    "\n",
    "    data['ID'] = data['ID'].apply(Available)\n",
    "    data = data.dropna(how='any')\n",
    "    print(len(data),'<- ID 칼럼에서 noise제거후 길이' )\n",
    "\n",
    "    data['Domain'] = data['Domain'].apply(Available)\n",
    "    data = data.dropna(how='any')\n",
    "    print(len(data),'<- Domain 칼럼에서 noise제거후 길이')\n",
    "    \n",
    "    \n",
    "    save_path = './Noise_Clean_3Billion/'+char+'.pickle'\n",
    "    data.to_pickle(save_path)\n",
    "        \n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['ID','Domain', 'PW']\n",
    "path = './3Billion/numbers/num.csv'\n",
    "\n",
    "data = pd.read_csv(path, names = colnames, header=None, error_bad_lines=False)\n",
    "print(len(data), '<- data의 원래 길이:' )\n",
    "\n",
    "data = tmp.dropna(axis=0)\n",
    "print(len(data),'<-  NaN제거후의 길이' )\n",
    "\n",
    "data['PW'] = data['PW'].apply(Available)\n",
    "data = data.dropna(how='any')\n",
    "print(len(data),'<- PW 칼럼에서 noise제거후 길이' )\n",
    "\n",
    "data['ID'] = data['ID'].apply(Available)\n",
    "data = data.dropna(how='any')\n",
    "print(len(data),'<- ID 칼럼에서 noise제거후 길이' )\n",
    "\n",
    "data['Domain'] = data['Domain'].apply(Available)\n",
    "data = data.dropna(how='any')\n",
    "print(len(data),'<- Domain 칼럼에서 noise제거후 길이')\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# save\n",
    "data.to_pickle('./Noise_Clean_3Billion/num.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for char in string.ascii_letters:\n",
    "    \n",
    "    \n",
    "    path = './Noise_Clean_3Billion/' +char + '.pickle'\n",
    "    #print(path)\n",
    "    data = pd.read_pickle(path)\n",
    "    name = char +'.pickle:'\n",
    "    print(name, len(data))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './num' + '.pickle'\n",
    "data = pd.read_pickle(path)\n",
    "name = 'num' +'.pickle:'\n",
    "print(name, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import string\n",
    "from datasketch import MinHash, MinHashLSHForest\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "from pyjarowinkler import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    num_gram = 4\n",
    "    tokens = [text[i:i+num_gram] for i in range(len(text)-num_gram+1)]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def get_forest(data, perms):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    minhash = []\n",
    "    \n",
    "    for text in data['ID']:\n",
    "        tokens = preprocess(text)\n",
    "        m = MinHash(num_perm=perms)\n",
    "        for s in tokens:\n",
    "            m.update(s.encode('utf8'))\n",
    "        minhash.append(m)\n",
    "        \n",
    "    forest = MinHashLSHForest(num_perm=perms)\n",
    "    \n",
    "    for i,m in enumerate(minhash):\n",
    "        forest.add(i,m)\n",
    "        \n",
    "    forest.index()\n",
    "    \n",
    "    print('It took %s seconds to build forest.' %(time.time()-start_time))\n",
    "    \n",
    "    return forest\n",
    "\n",
    "#def predict(text, database, perms, num_results, forest):\n",
    "#def predict(text, database, perms, num_results, forest):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    m = MinHash(num_perm=perms)\n",
    "    for s in tokens:\n",
    "        m.update(s.encode('utf8'))\n",
    "        \n",
    "    idx_array = np.array(forest.query(m, num_results))\n",
    "    if len(idx_array) == 0:\n",
    "        return None # if your query is empty, return none\n",
    "    \n",
    "    #result = database.iloc[idx_array]['id']\n",
    "    #result_pw = database.iloc[idx_array]['password']\n",
    "    result = idx_array\n",
    "    \n",
    "    #print('It took %s seconds to query forest.' %(time.time()-start_time))\n",
    "    \n",
    "    return result\n",
    "\n",
    "def predict(text, perms, num_results, forest):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tokens = preprocess(text)\n",
    "    m = MinHash(num_perm=perms)\n",
    "    for s in tokens:\n",
    "        m.update(s.encode('utf8'))\n",
    "        \n",
    "    idx_array = np.array(forest.query(m, num_results))\n",
    "    if len(idx_array) == 0:\n",
    "        return None # if your query is empty, return none\n",
    "    \n",
    "    result = idx_array\n",
    "    \n",
    "    return result\n",
    "\n",
    "def hash_list(values):\n",
    "    total = '0'\n",
    "    for i in range(len(values)):\n",
    "        total += values[i]    \n",
    "    result = hash(total)\n",
    "    #print(total)\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def lsh(num):\n",
    "    \n",
    "    a= data['ID'][num]\n",
    "    b = data['PW'][num]\n",
    "    refer = [a,b]\n",
    "    \n",
    "    idx = num\n",
    "    compare_id = refer[0]\n",
    "    \n",
    "    predict_result = predict(compare_id, permutations, num_recommendations, forest)\n",
    "    \n",
    "    #lsh_data = data.iloc[sorted(predict_result)]    \n",
    "    return(predict_result)\n",
    "\n",
    "def Hashing(hash_list):\n",
    "    total = '0'\n",
    "    for i in range(len(hash_list)):\n",
    "        total += hash_list[i]    \n",
    "    result = hash(total)\n",
    "    #print(total)\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def PWset(idx):\n",
    "    reference = data['PW'][idx]\n",
    "    \n",
    "    result  = lsh(idx)\n",
    "    temp_df = data.loc[sorted(result)]\n",
    "    \n",
    "    PW = list(temp_df['PW'].values)\n",
    "    PWset = list()\n",
    "    \n",
    "    for password in PW:\n",
    "        temp = distance.get_jaro_distance(reference, password)\n",
    "        #print(reference, password)\n",
    "        if 1>= temp > threshold:\n",
    "            PWset.append(password)\n",
    "    PWset = list(set(PWset))\n",
    "    \n",
    "    if len(PWset)>1:\n",
    "        return PWset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveForest(forest, seq):\n",
    "    \n",
    "    save_path = './LSH_Pickle/'+'s/'+'forest_s_'+str(seq) +'.pickle'\n",
    "    with open (save_path, 'wb') as f:\n",
    "        pickle.dump(forest,f)\n",
    "    print(seq, 'forest_saved')\n",
    "    \n",
    "def SavePWset(data, seq):\n",
    "        \n",
    "    data = data.dropna(how='any')\n",
    "    data['HashTag'] = data['PWset'].apply(Hashing)\n",
    "    data = data.drop_duplicates(subset='HashTag', keep ='first')\n",
    "    \n",
    "    \n",
    "    save_path = './LSH_Data/'+'s/'+'s_'+str(seq) +'.pickle'\n",
    "    data.to_pickle(save_path)\n",
    "    print(seq,'Data Pickle Saved')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Permutations\n",
    "\n",
    "threshold = 0.8\n",
    "permutations = 32\n",
    "num_recommendations = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    \n",
    "    path = './For_LSH_data/s/'+'s_'+str(i)+'.pickle'\n",
    "    data = pd.read_pickle(path)\n",
    "    print(data.head())\n",
    "    \n",
    "    forest = get_forest(data, permutations)\n",
    "    \n",
    "    SaveForest(forest,i)\n",
    "    \n",
    "    data = data.assign(index=lambda x: data.index)\n",
    "    start = time.time()\n",
    "    data['PWset'] = data['index'].apply(PWset)\n",
    "    current = time.time() - start\n",
    "    print('Making PWset consumed time:' ,current)\n",
    "    \n",
    "    SavePWset(data, i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
